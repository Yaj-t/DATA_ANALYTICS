{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['hair', 'feathers', 'eggs', 'milk', 'airborne', 'aquatic', 'predator',\n",
      "       'toothed', 'backbone', 'breathes', 'venomous', 'fins', 'legs', 'tail',\n",
      "       'domestic', 'catsize', 'type'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from scipy.special import logsumexp\n",
    "import numpy as np\n",
    "\n",
    "# Fetch dataset\n",
    "zoo = fetch_ucirepo(id=111)\n",
    "X = zoo.data.features\n",
    "y = zoo.data.targets \n",
    "zoo_df = pd.merge(X, y, left_index=True, right_index=True)\n",
    "\n",
    "# Inspect the dataframe to check the column names\n",
    "print(zoo_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46.25904501111759\n",
      "Init: initializing centroids\n",
      "Init: initializing clusters\n",
      "Starting iterations...\n",
      "Run 1, iteration: 1/100, moves: 14, cost: 174.0\n",
      "Run 1, iteration: 2/100, moves: 1, cost: 174.0\n",
      "Init: initializing centroids\n",
      "Init: initializing clusters\n",
      "Starting iterations...\n",
      "Run 2, iteration: 1/100, moves: 20, cost: 143.0\n",
      "Run 2, iteration: 2/100, moves: 1, cost: 143.0\n",
      "Init: initializing centroids\n",
      "Init: initializing clusters\n",
      "Starting iterations...\n",
      "Run 3, iteration: 1/100, moves: 18, cost: 160.0\n",
      "Run 3, iteration: 2/100, moves: 3, cost: 160.0\n",
      "Init: initializing centroids\n",
      "Init: initializing clusters\n",
      "Starting iterations...\n",
      "Run 4, iteration: 1/100, moves: 13, cost: 192.0\n",
      "Run 4, iteration: 2/100, moves: 0, cost: 192.0\n",
      "Init: initializing centroids\n",
      "Init: initializing clusters\n",
      "Starting iterations...\n",
      "Run 5, iteration: 1/100, moves: 27, cost: 139.0\n",
      "Run 5, iteration: 2/100, moves: 2, cost: 139.0\n",
      "Best run was number 5\n",
      "Bernoulli Mixture with Stochastic EM Algorithm - ARI: 0.0, NMI: 0.0, FMI: 0.48277252089435774\n",
      "K-Modes - ARI: 0.5458190715381364, NMI: 0.7308325877325337, FMI: 0.6437102028203917\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Assuming the target column is not 'class', find the correct column name\n",
    "true_labels = zoo_df['type']  # replace 'animal_name' with the actual column name of your target\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "# Drop any rows with missing values\n",
    "zoo_df = zoo_df.dropna()\n",
    "\n",
    "# Define the Bernoulli Mixture with Stochastic EM Algorithm class\n",
    "class BernoulliMixturewSEM:\n",
    "    \n",
    "    def __init__(self, n_components, max_iter, batch_size=10, tol=1e-3):\n",
    "        self.n_components = n_components\n",
    "        self.max_iter = max_iter\n",
    "        self.batch_size = batch_size\n",
    "        self.tol = tol\n",
    "    \n",
    "    def fit(self, x):\n",
    "        self.x = x\n",
    "        self.init_params()\n",
    "        log_bernoullis = self.get_log_bernoullis(self.x)\n",
    "        self.old_logL = self.get_log_likelihood(log_bernoullis)\n",
    "        for step in range(self.max_iter):\n",
    "            if step > 0:\n",
    "                self.old_logL = self.logL\n",
    "            for batch in self.iterate_batches():\n",
    "                self.gamma = self.get_responsibilities(self.get_log_bernoullis(batch))\n",
    "                self.remember_params()\n",
    "                self.get_Neff()\n",
    "                self.get_mu(batch)\n",
    "                self.get_pi()\n",
    "            log_bernoullis = self.get_log_bernoullis(self.x)\n",
    "            self.logL = self.get_log_likelihood(log_bernoullis)\n",
    "            if np.abs(self.logL - self.old_logL) < self.tol:\n",
    "                break\n",
    "            if np.isnan(self.logL):\n",
    "                self.reset_params()\n",
    "                print(self.logL)\n",
    "                break\n",
    "\n",
    "    def iterate_batches(self):\n",
    "        n_samples = len(self.x)\n",
    "        indices = np.arange(n_samples)\n",
    "        np.random.shuffle(indices)\n",
    "        for start_idx in range(0, n_samples, self.batch_size):\n",
    "            end_idx = min(start_idx + self.batch_size, n_samples)\n",
    "            batch_indices = indices[start_idx:end_idx]\n",
    "            yield self.x.iloc[batch_indices]\n",
    "\n",
    "    def reset_params(self):\n",
    "        self.mu = self.old_mu.copy()\n",
    "        self.pi = self.old_pi.copy()\n",
    "        self.gamma = self.old_gamma.copy()\n",
    "        self.get_Neff()\n",
    "        log_bernoullis = self.get_log_bernoullis(self.x)\n",
    "        self.logL = self.get_log_likelihood(log_bernoullis)\n",
    "        \n",
    "    def remember_params(self):\n",
    "        self.old_mu = self.mu.copy()\n",
    "        self.old_pi = self.pi.copy()\n",
    "        self.old_gamma = self.gamma.copy()\n",
    "    \n",
    "    def init_params(self):\n",
    "        self.n_samples = self.x.shape[0]\n",
    "        self.n_features = self.x.shape[1]\n",
    "        self.pi = 1/self.n_components * np.ones(self.n_components)\n",
    "        self.mu = np.random.RandomState(seed=0).uniform(low=0.25, high=0.75, size=(self.n_components, self.n_features))\n",
    "        self.normalize_mu()\n",
    "        self.old_mu = None\n",
    "        self.old_pi = None\n",
    "        self.old_gamma = None\n",
    "    \n",
    "    def normalize_mu(self):\n",
    "        sum_over_features = np.sum(self.mu, axis=1)\n",
    "        for k in range(self.n_components):\n",
    "            self.mu[k,:] /= sum_over_features[k]\n",
    "            \n",
    "    def get_responsibilities(self, log_bernoullis):\n",
    "        gamma = np.zeros(shape=(log_bernoullis.shape[0], self.n_components))\n",
    "        Z =  logsumexp(np.log(self.pi[None,:]) + log_bernoullis, axis=1)\n",
    "        for k in range(self.n_components):\n",
    "            gamma[:, k] = np.exp(np.log(self.pi[k]) + log_bernoullis[:,k] - Z)\n",
    "        return gamma\n",
    "        \n",
    "    def get_log_bernoullis(self, x):\n",
    "        log_bernoullis = self.get_save_single(x, self.mu)\n",
    "        log_bernoullis += self.get_save_single(1-x, 1-self.mu)\n",
    "        return log_bernoullis\n",
    "    \n",
    "    def get_save_single(self, x, mu):\n",
    "        epsilon = 1e-15\n",
    "        mu_place = np.clip(mu, epsilon, 1 - epsilon)\n",
    "        return np.tensordot(x, np.log(mu_place), (1,1))\n",
    "\n",
    "    def get_Neff(self):\n",
    "        self.Neff = np.sum(self.gamma, axis=0)\n",
    "    \n",
    "    def get_mu(self, batch):\n",
    "        self.mu = np.einsum('ik,id -> kd', self.gamma, batch) / self.Neff[:, None]\n",
    "        \n",
    "    def get_pi(self):\n",
    "        self.pi = self.Neff / self.n_samples\n",
    "    \n",
    "    def predict(self, x):\n",
    "        log_bernoullis = self.get_log_bernoullis(x)\n",
    "        gamma = self.get_responsibilities(log_bernoullis)\n",
    "        return np.argmax(gamma, axis=1)\n",
    "        \n",
    "    def get_sample_log_likelihood(self, log_bernoullis):\n",
    "        return logsumexp(np.log(self.pi[None,:]) + log_bernoullis, axis=1)\n",
    "    \n",
    "    def get_log_likelihood(self, log_bernoullis):\n",
    "        return np.mean(self.get_sample_log_likelihood(log_bernoullis))\n",
    "        \n",
    "    def score(self, x):\n",
    "        log_bernoullis = self.get_log_bernoullis(x)\n",
    "        return self.get_log_likelihood(log_bernoullis)\n",
    "    \n",
    "    def score_samples(self, x):\n",
    "        log_bernoullis = self.get_log_bernoullis(x)\n",
    "        return self.get_sample_log_likelihood(log_bernoullis)\n",
    "\n",
    "# Perform clustering and comparison\n",
    "# Ensure you calculate FMI, ARI, and NMI scores for comparison\n",
    "\n",
    "# Sample code for calculating performance metrics\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "from sklearn.metrics.cluster import fowlkes_mallows_score\n",
    "\n",
    "# Assuming 'true_labels' contains the ground truth labels\n",
    "true_labels = zoo_df['type']  \n",
    "# Clustering with Bernoulli Mixture with Stochastic EM Algorithm\n",
    "bmm = BernoulliMixturewSEM(n_components=7, max_iter=100)\n",
    "bmm.fit(X)\n",
    "predicted_labels_bmm = bmm.predict(X)\n",
    "\n",
    "# Clustering with K-Modes\n",
    "from kmodes.kmodes import KModes\n",
    "km = KModes(n_clusters=7, init='Huang', n_init=5, verbose=1)\n",
    "predicted_labels_kmodes = km.fit_predict(X)\n",
    "\n",
    "# Calculate performance metrics\n",
    "ari_bmm = adjusted_rand_score(true_labels, predicted_labels_bmm)\n",
    "nmi_bmm = normalized_mutual_info_score(true_labels, predicted_labels_bmm)\n",
    "fmi_bmm = fowlkes_mallows_score(true_labels, predicted_labels_bmm)\n",
    "\n",
    "ari_kmodes = adjusted_rand_score(true_labels, predicted_labels_kmodes)\n",
    "nmi_kmodes = normalized_mutual_info_score(true_labels, predicted_labels_kmodes)\n",
    "fmi_kmodes = fowlkes_mallows_score(true_labels, predicted_labels_kmodes)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Bernoulli Mixture with Stochastic EM Algorithm - ARI: {ari_bmm}, NMI: {nmi_bmm}, FMI: {fmi_bmm}\")\n",
    "print(f\"K-Modes - ARI: {ari_kmodes}, NMI: {nmi_kmodes}, FMI: {fmi_kmodes}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>The Folkes-Mallows Index (FMI) is often higher than the Adjusted Rand Index (ARI) and Normalized Mutual Information Score (NMI) because it focuses solely on the similarity between clusters. FMI measures the geometric mean of precision and recall for pairs of points that are in the same or different clusters in the true and predicted clusterings.</p>\n",
    "\n",
    "<p>In contrast, ARI and NMI consider both similarity and dissimilarity. ARI is adjusted for chance and takes into account the agreement between cluster pairs relative to random labeling. NMI measures the amount of information shared between the true and predicted clusterings, normalizing the mutual information to account for the entropy of the clusterings.</p>\n",
    "\n",
    "<p>Therefore, FMI tends to be higher because it is less sensitive to the noise and intra-cluster variations, whereas ARI and NMI might be lower due to their adjustments and consideration of both cluster similarity and dissimilarity.</p>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
